{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_Keras_MLP_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvhdciXFz-PI",
        "colab_type": "text"
      },
      "source": [
        "## MLP Structures\n",
        "- Each MLP model is consisted of one input layer, several hidden layers, and one output layer\n",
        "- Number of neurons in each layer is not limited\n",
        "<img src=\"http://cs231n.github.io/assets/nn1/neural_net.jpeg\" style=\"width: 300px\"/>\n",
        "<br>\n",
        "\n",
        "## MLP with one hidden layer\n",
        "- Number of input neurons: 3\n",
        "- Number of hidden neurons: 4\n",
        "- Number of output neurons: 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhGwsqNV0K2V",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" style=\"width: 500px\"/>\n",
        "<br>\n",
        "\n",
        "## MLP with two hidden layers\n",
        "- Number of input neurons: 3\n",
        "- Number of hidden neurons: (4, 4)\n",
        "- Number of output neurons: 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEO00u_d0egP",
        "colab_type": "text"
      },
      "source": [
        "## MLP for regression tasks.\n",
        "- When you have to estimate/predict continuous real-valued numbers.\n",
        "(Technically, when target function(y) is real valued)\n",
        "\n",
        "- Commonly used loss function is means-squared-error (mse)\n",
        "\n",
        "We shall the application on boston house prices dataset. <br>\n",
        "This is included in keras library. (https://keras.io/api/datasets/boston_housing/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBl8NR9Bz2cL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cbfff2e3-3357-462c-fb37-c00f73496edb"
      },
      "source": [
        "## Fetch dataset\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data(test_split = 0.3, seed = 2020)\n",
        "\n",
        "## test_split parameter here says 30% of dataset should be kept for testing, so 70% will be used for training\n",
        "## seed is used to ensure the random split is reproducible"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S873pJDo2IHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b2d9b6f9-37ee-4814-e89e-79c6fdb1a3bd"
      },
      "source": [
        "## Examine the data\n",
        "print(X_train.shape, X_test .shape)\n",
        "\n",
        "## So we have 13 attributes or features as input for each sample.\n",
        "print(y_train.shape, y_test.shape)\n",
        "\n",
        "## Only one value has to be predicted as the target. \n",
        "\n",
        "## From the documentation using above link, we know that we are predicting median value of a house at a location in 1000s of $\n",
        "print(np.amax(y_train), np.amin(y_train))\n",
        "## We  can see that min value in training set is 5.0 and max value is 50.0 (in k$) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(354, 13) (152, 13)\n",
            "(354,) (152,)\n",
            "50.0 5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWwGfREWFf8Q",
        "colab_type": "text"
      },
      "source": [
        "### Input normalization (aka Feature scaling):\n",
        "- In this case, all features are continuous valued, so we can normalize them as shown here.\n",
        "- But we shouldn't blindly normalize them in this way for any data. (Need to take care of categorical values and missing values)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-1TKaL1-wUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Normalize inputs\n",
        "X_train_mean = np.mean(X_train, axis = 0)\n",
        "X_train_std = np.std(X_train, axis = 0)\n",
        "X_train_normd = (X_train - X_train_mean)/(10**-8 + X_train_std)  ## adding a small number to denominator to avoid division by 0 errors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIeCmcsb4LEQ",
        "colab_type": "text"
      },
      "source": [
        "### Creating a model with Sequential class\n",
        "- ```Fully connected``` layers are called ```Dense``` layers in keras\n",
        "- We shall use ```relu``` activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jthqZZZu2vZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "b8226ac6-6945-4b0f-d89b-1719a3694910"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "model = Sequential()  ## Initialize an empty sequential model\n",
        "model.add(Input(shape = (13, )))   ## First add input layer, specify shape of 1 input sample\n",
        "model.add(Dense(4, activation = 'relu'))  ## Add a hidden layer with 16 neurons, relu activation\n",
        "model.add(Dense(2, activation = 'relu'))   ## Add a hidden layer with 8 neurons, relu activation\n",
        "model.add(Dense(1, activation = 'relu'))   ## Add ouptut layer with 1 neuron (because we need to predict a single value), relu activation\n",
        "\n",
        "## Look at summary of defined model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4)                 56        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 10        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 69\n",
            "Trainable params: 69\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcoI3u_Q5J85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Now compile the model - add optimizer, loss and metrics to track\n",
        "model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.01), loss = 'mean_squared_error', metrics = ['mse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APGomT4V5eqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e8b336d-51ce-4310-d843-11e3b449dc9e"
      },
      "source": [
        "## Now fit the model to training data - specify epochs, batch_size, validation_split\n",
        "history = model.fit(X_train_normd, y_train, batch_size = 32, epochs = 100, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 0s 26ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3991 - mse: 591.3991 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3991 - mse: 591.3991 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3991 - mse: 591.3991 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3991 - mse: 591.3991 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3991 - mse: 591.3991 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3991 - mse: 591.3991 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3991 - mse: 591.3991 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3991 - mse: 591.3991 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3991 - mse: 591.3991 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 591.3992 - mse: 591.3992 - val_loss: 698.3755 - val_mse: 698.3755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3wPiq78AUT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "ee4288cb-066e-4543-ba8c-86df59de5103"
      },
      "source": [
        "val_mse = history.history['val_mse']  ##The history attribute is a dictionary which stores metrics from each epoch\n",
        "mse = history.history['mse']\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(val_mse, color = 'red')  ## Plot the validation error\n",
        "plt.plot(mse, color = 'blue') ## Plot training error\n",
        "## You can see that the validation error saturate after 25 epochs or so. We don't need to train till 100 epochs (Early stopping)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe8753f1390>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrLxa6-JAV5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f0c05c4c-4683-4319-fbd6-650fa8b52a78"
      },
      "source": [
        "## Now evaluate the model on test data\n",
        "## But first normalize test data using same normalization that was used for training data\n",
        "\n",
        "X_test_normd = (X_test - X_train_mean)/(10**-8 + X_train_std)\n",
        "model.evaluate(X_test_normd, y_test)\n",
        "\n",
        "\n",
        "## Mean squared error is 10 (Means our prediction for median house price is off by $3k (square root of 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 2ms/step - loss: 568.5519 - mse: 568.5519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[568.5518798828125, 568.5518798828125]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvkQFs1LCdOR",
        "colab_type": "text"
      },
      "source": [
        "## Some things to explore\n",
        "- Try to see training performance if you don't normalize the inputs\n",
        "- Try to tune the hyperparameters (number of layers, number of neurons, learning_rate of optimizer, batch_size etc) to see if you can converge faster or get a lower mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggRAKsMbDJib",
        "colab_type": "text"
      },
      "source": [
        "## Assignment\n",
        "- Try to use these techniques to model power plant data \n",
        "https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMhz4q-SFBPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}